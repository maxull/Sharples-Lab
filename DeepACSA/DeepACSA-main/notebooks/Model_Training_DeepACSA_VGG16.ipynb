{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from skimage.io import imshow\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn import datasets\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.backend import clear_session\n",
    "from keras import backend as K\n",
    "from keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
    "from tensorflow.keras.applications import VGG16 \n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test whether GPU is present and recognized\n",
    "# Limit memory allocation\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.set_logical_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.LogicalDeviceConfiguration(memory_limit=4096)])\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(inputs, num_filters):\n",
    "    x = Conv2D(num_filters, 3, padding = \"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    x = Conv2D(num_filters, 3, padding = \"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def decoder_block(inputs, skip_features, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(inputs) #32\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def build_vgg16_unet(input_shape):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    vgg16 = VGG16(include_top=False, weights=\"imagenet\", input_tensor = inputs)\n",
    "    #vgg16.summary()\n",
    "    \n",
    "    \"\"\" Encoder \"\"\"\n",
    "    \n",
    "    # skip connections\n",
    "    s1 = vgg16.get_layer(\"block1_conv2\").output # 256\n",
    "    s2 = vgg16.get_layer(\"block2_conv2\").output # 128\n",
    "    s3 = vgg16.get_layer(\"block3_conv3\").output # 64\n",
    "    s4 = vgg16.get_layer(\"block4_conv3\").output # 32\n",
    "\n",
    "    \"\"\" Bottleneck/Bridge \"\"\"\n",
    "    \n",
    "    b1 = vgg16.get_layer(\"block5_conv3\").output # 16\n",
    "    \n",
    "    \"\"\" Decoder \"\"\"\n",
    "\n",
    "    d1 = decoder_block(b1, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "    \n",
    "    \"\"\" Outputs \"\"\"\n",
    "    outputs = Conv2D(1, (1, 1), padding = \"same\", activation=\"sigmoid\")(d4) #binary segmentation\n",
    "    model = Model(inputs, outputs, name = \"VGG16_U-Net\")\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution block\n",
    "def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n",
    "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
    "    # first layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size), \\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # second layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Create u-net model\n",
    "def get_unet(input_img, n_filters = 64, dropout = 0.1, batchnorm = True):\n",
    "    \"\"\"Function to define the UNET Model\"\"\"\n",
    "    \n",
    "    # Contracting Path\n",
    "    # c is output tensor of conv layers\n",
    "    # p ist output tensor of max pool layers\n",
    "    # u is output tensor of up-sampling (transposed) layers\n",
    "    # Batchnorm standardizes/normalizes the output of each layer where applied in order to avoid huge weights using \n",
    "    # z-scores \n",
    "    \n",
    "    c1 = conv2d_block(input_img, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    p1 = Dropout(dropout)(p1)\n",
    "    \n",
    "    c2 = conv2d_block(p1, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    p2 = Dropout(dropout)(p2)\n",
    "    \n",
    "    c3 = conv2d_block(p2, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "    p3 = Dropout(dropout)(p3)\n",
    "    \n",
    "    c4 = conv2d_block(p3, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "    p4 = Dropout(dropout)(p4)\n",
    "    \n",
    "    c5 = conv2d_block(p4, n_filters = n_filters * 16, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    # Expansive Path\n",
    "    u6 = Conv2DTranspose(n_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    u6 = Dropout(dropout)(u6)\n",
    "    c6 = conv2d_block(u6, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    u7 = Dropout(dropout)(u7)\n",
    "    c7 = conv2d_block(u7, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    u8 = Dropout(dropout)(u8)\n",
    "    c8 = conv2d_block(u8, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u9 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(c8)\n",
    "    u9 = concatenate([u9, c1])\n",
    "    u9 = Dropout(dropout)(u9)\n",
    "    c9 = conv2d_block(u9, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "    model = Model(inputs=[input_img], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "# Compute Intersection over union (IoU), a measure of labelling accuracy\n",
    "# NOTE: This is sometimes also called Jaccard score\n",
    "def IoU(y_true, y_pred, smooth=1):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    union = K.sum(y_true,-1) + K.sum(y_pred,-1) - intersection\n",
    "    iou = (intersection + smooth) / ( union + smooth)\n",
    "    return iou\n",
    "\n",
    "def dice_score(y_true, y_pred, smooth=1):\n",
    "    \n",
    "    # Flatten\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    \n",
    "    return 1 - dice_score(y_true, y_pred)\n",
    "\n",
    "def dice_bce_score(y_true, y_pred, smooth=1):    \n",
    "    \n",
    "    BCE =  K.binary_crossentropy(y_true, y_pred)\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)    \n",
    "    dice_loss = 1 - (2*intersection + smooth) / (K.sum(y_true, -1) + K.sum(y_pred, -1) + smooth)\n",
    "    Dice_BCE = BCE + dice_loss\n",
    "    \n",
    "    return Dice_BCE\n",
    "\n",
    "def focal_loss(y_true, y_pred, alpha=0.8, gamma=2):    \n",
    "      \n",
    "    BCE = K.binary_crossentropy(y_true, y_pred)\n",
    "    BCE_EXP = K.exp(-BCE)\n",
    "    focal_loss = K.mean(alpha * K.pow((1-BCE_EXP), gamma) * BCE)\n",
    "    return focal_loss\n",
    "\n",
    "# Plot sample of model prediction\n",
    "def plot_sample(X, y, preds, binary_preds, ix=None):\n",
    "    if ix is None:\n",
    "        ix = random.randint(0, len(X))\n",
    "\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(30, 20))\n",
    "    ax[0].imshow(X[ix, ..., 0], cmap='Greys_r')\n",
    "    \n",
    "    ax[0].set_title('US-image', c=\"white\" )\n",
    "    ax[0].grid(False)\n",
    "\n",
    "    ax[1].imshow(y[ix].squeeze(), cmap='Greys_r')\n",
    "    ax[1].set_title('Aponeurosis', c=\"white\")\n",
    "    ax[1].grid(False)\n",
    "\n",
    "    ax[2].imshow(preds[ix].squeeze(), vmin=0, vmax=1, cmap=\"Greys_r\")\n",
    "    \n",
    "    ax[2].set_title('Apo-Predicted', c=\"white\")\n",
    "    ax[2].grid(False)\n",
    "    \n",
    "    ax[3].imshow(binary_preds[ix].squeeze(), vmin=0, vmax=0.5, cmap=\"Greys_r\")\n",
    "    \n",
    "    ax[3].set_title('Apo-Picture binary', c=\"white\")\n",
    "    ax[3].grid(False)\n",
    "    \n",
    "    plt.savefig(str(ix)+\"Pred_area.tif\")\n",
    "\n",
    "# Save all predictions on disk \n",
    "def save_pred_area(binary_preds): \n",
    "    for i in range(len(binary_preds)): \n",
    "        fig, (ax1)= plt.subplots(1, 1, figsize = (15, 15))\n",
    "        ax1.imshow(binary_preds[i], cmap=\"Greys_r\", interpolation=\"bilinear\")\n",
    "        ax1.set_title(\"Predicted Area\")\n",
    "        plt.savefig(str(i)+\"Pred_area.tif\") # Saves images to directory of notebook\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMAGE AUGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use only when training new models and not enough data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input image & mask folder variables and prefix for saved augmented images\n",
    "# Change before augmentation\n",
    "input_img_folder = \"../DeepACSA_Training_Images/apo_image_csa_Gastro/insert_images/\"\n",
    "input_mask_folder = \"../DeepACSA_Training_Images/apo_masks_csa_Gastro/insert_masks/\"\n",
    "img_aug_prefix = \"gastroc\"\n",
    "\n",
    "# Creating image augmentation function\n",
    "gen = ImageDataGenerator(featurewise_center=True,\n",
    "                        featurewise_std_normalization=True,\n",
    "                        rotation_range=5, \n",
    "                        width_shift_range=0.075, \n",
    "                        height_shift_range=0.075,\n",
    "                        zoom_range=0.075,\n",
    "                        horizontal_flip=True)\n",
    "\n",
    "ids = os.listdir(input_mask_folder)\n",
    "seed = 131313\n",
    "batch_size = 1\n",
    "num_aug_images = 5 # Number of images added from augmented dataset. \n",
    "\n",
    "\n",
    "for i in range(int(len(ids) + 1)):\n",
    "    \n",
    "    # Choose image & mask that should be augmented \n",
    "    # Directory structur: \"root/some_dorectory/\"\n",
    "    chosen_image = ids[i] \n",
    "    image_path = input_img_folder + chosen_image \n",
    "    mask_path = input_mask_folder + chosen_image\n",
    "    image = np.expand_dims(plt.imread(image_path),0)# Read and expand image dimensions\n",
    "    if image.ndim < 4: \n",
    "        image = np.expand_dims(image,-1)\n",
    "    mask = np.expand_dims(plt.imread(mask_path),0)\n",
    "    if mask.ndim < 4: \n",
    "        mask = np.expand_dims(mask,-1)\n",
    "\n",
    "    # Augment images \n",
    "    aug_image = gen.flow(image, batch_size=batch_size, seed=seed, save_to_dir=input_img_folder, save_prefix=img_aug_prefix+str(i), save_format=\"tif\")\n",
    "    aug_mask = gen.flow(mask, batch_size=batch_size, seed=seed, save_to_dir=input_mask_folder, save_prefix=img_aug_prefix+str(i), save_format=\"tif\")\n",
    "    seed = seed + 1 \n",
    "     \n",
    "    # Add images to folder\n",
    "    for i in range(num_aug_images):\n",
    "        next(aug_image)[0].astype(np.uint8)\n",
    "        next(aug_mask)[0].astype(np.uint8)\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APONEUROSIS TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set image scaling parameters, determine no. of images for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images will be re-scaled\n",
    "im_width = 256\n",
    "im_height = 256\n",
    "border = 5\n",
    "\n",
    "# list of all images in the path\n",
    "ids = os.listdir(\"../DeepACSA_Training_Images/apo_image_csa_VL/insert_images/\")\n",
    "print(\"Total no. of aponeurosis images = \", len(ids))\n",
    "X = np.zeros((len(ids), im_height, im_width, 1), dtype=np.float32)\n",
    "y = np.zeros((len(ids), im_height, im_width, 1), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load images and corresponding labels (masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = []\n",
    "for path in glob.glob(\"../DeepACSA_Training_Images/apo_image_csa_Gastro/insert_images\"):\n",
    "    for img_path in glob.glob(os.path.join(path, \"*.tif\")):\n",
    "        img = cv2.imread(img_path, 1)\n",
    "        img = cv2.resize(img, (256,256))\n",
    "        img = img_to_array(img)\n",
    "        img = img/255.0\n",
    "        train_images.append(img)  \n",
    "train_images = np.array(train_images)\n",
    "\n",
    "train_masks = []\n",
    "for path in glob.glob(\"../DeepACSA_Training_Images/apo_masks_csa_Gastro/insert_masks\"):\n",
    "    for mask_path in glob.glob(os.path.join(path, \"*.tif\")):\n",
    "        mask = cv2.imread(mask_path, 0)\n",
    "        mask = cv2.resize(mask, (256,256))\n",
    "        mask = img_to_array(mask)\n",
    "        mask = mask/255.0\n",
    "        train_masks.append(mask)\n",
    "        \n",
    "train_masks = np.array(train_masks)\n",
    "train_masks = np.expand_dims(train_masks, axis=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up aponeurosis training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_images, train_masks, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a random image along with the mask (not necessary, just for checking)\n",
    "ix = random.randint(0, len(X_train))\n",
    "has_mask = y_train[ix].max() > 0 # Check whether there's at least 1 aponeurosis\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (15, 12))\n",
    "ax1.imshow(X_train[ix, ..., 0], cmap = 'gray', interpolation = 'bilinear')\n",
    "if has_mask: # if at least 1 aponeurosis is present\n",
    "    #draw the aponeuroses on the original image\n",
    "    #ax1.contour(y_train[ix].squeeze(), colors = 'k', linewidths = 0, levels = [0.5])\n",
    "    ax1.set_title('Original image')\n",
    "    ax1.grid(False)\n",
    "    ax2.imshow(y_train[ix].squeeze(), cmap = 'gray', interpolation = 'bilinear')\n",
    "    ax2.set_title('Mask only')\n",
    "    ax2.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the aponeurosis model VGG16\n",
    "VGG16_UNet = build_vgg16_unet((256,256,3))\n",
    "model_apo = VGG16_UNet\n",
    "model_apo.compile(optimizer=Adam(), loss=dice_bce_score, metrics=[\"accuracy\", IoU])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some training parameters\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=8, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint('VGG16preDiceBCE-RF-256.h5', verbose=1, save_best_only=True, save_weights_only=False), # Give the model a name (the .h5 part)\n",
    "    CSVLogger('VGG16preDiceBCE-RF-256.csv', separator=',', append=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the aponeurosis model (keep batch size small!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_apo.fit(X_train, y_train, batch_size=1, epochs=60,\n",
    "                       callbacks=callbacks, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the results of training\n",
    "# Variables stored in results.history: val_loss, val_acc, val_IoU, loss, acc, IoU, lr\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 8))\n",
    "ax[0].plot(results.history[\"loss\"], label=\"Training loss\")\n",
    "ax[0].plot(results.history[\"val_loss\"], label=\"Validation loss\")\n",
    "ax[0].set_title('Learning curve')\n",
    "ax[0].plot( np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "ax[0].set_ylabel(\"log_loss\")\n",
    "ax[0].legend();\n",
    "\n",
    "ax[1].plot(results.history[\"val_IoU\"], label=\"Training IoU\")\n",
    "ax[1].plot(results.history[\"IoU\"], label=\"Validation IoU\")\n",
    "ax[1].set_title(\"IoU curve\")\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "ax[1].set_ylabel(\"IoU score\")\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate on validation set (loss, acc, IoU)\n",
    "model_apo.evaluate(X_valid, y_valid, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on training and validations sets\n",
    "preds_train = model_apo.predict(X_train, verbose=1)\n",
    "preds_val = model_apo.predict(X_valid, verbose=1)\n",
    "\n",
    "# Threshold predictions (only keep predictions with a minimum level of confidence)\n",
    "preds_train_t = (preds_train > 0.5).astype(np.uint8)\n",
    "preds_val_t = (preds_val > 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.history # Show the loss values (these are saved to a .csv file using 'CSVLogger' callback defined above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_apo = load_model(\"VGG16pre-Gastro-256.h5\", custom_objects={'IoU': IoU})\n",
    "model_apo.evaluate(X_valid, y_valid, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
